{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fb96c7",
   "metadata": {},
   "source": [
    "# COMP 352 - Final Project: Footballer Market Evaluation\n",
    "\n",
    "**Author:** Zevin Attisha, Santi Guerrero, Santiago Pedetti, Christian Vaikona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a986aa",
   "metadata": {},
   "source": [
    "#### Full dataset: https://www.kaggle.com/datasets/davidcariboo/player-scores/data?select=appearances.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fcb23",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Environment Setup](#env-setup)\n",
    "* [Data Importing and Pre-processing](#data-importing)\n",
    "* [Data Analysis and Visualization](#data-vis)\n",
    "* [Data Analytics](#data-analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac7f6f",
   "metadata": {},
   "source": [
    "## Environment Setup <a class=\"anchor\" id=\"env-setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1dbaf5",
   "metadata": {},
   "source": [
    "(COPIED AND PASTED - EDIT LATER)\n",
    "\n",
    "First we must setup our environment to make sure we have all appropriate modules installed. To do this, I have provided 2 methods. The 1st, is to install all modules using a ```.yaml``` file via ```conda```. \n",
    "\n",
    "To do this, run:\n",
    "```bash\n",
    "conda env create -f env_setup/data_environment.yml\n",
    "```\n",
    "Then activate the environment by:\n",
    "```bash\n",
    "conda activate data_env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e091322",
   "metadata": {},
   "source": [
    "(COPIED AND PASTED - EDIT LATER)\n",
    "\n",
    "You can also use the ```requirements.txt``` file to download the modules via ```pip```.\n",
    "\n",
    "To do this, first make create and activate your environment:\n",
    "```bash\n",
    "conda create -n my_data_env\n",
    "conda activate my_data_env\n",
    "```\n",
    "\n",
    "You may need to install setup tools. To do this run (Note you may need to change ```pip3``` to ```pip```):\n",
    "```bash\n",
    "pip3 install --upgrade pip setuptools wheel\n",
    "```\n",
    "\n",
    "and then run:\n",
    "```bash\n",
    "pip3 install -r env_setup/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7c2e2",
   "metadata": {},
   "source": [
    "## Data Importing and Pre-processing <a class=\"anchor\" id=\"data-importing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ac1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeRegressor\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_scripts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hello\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetric_scripts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hello\n\u001b[32m     29\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# import libraries needed\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import warnings\n",
    "\n",
    "import branca\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from branca.element import Figure\n",
    "from folium import Marker\n",
    "from folium.plugins import HeatMap\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import norm, probplot, skew\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from utils.model_scripts import (\n",
    "    hello\n",
    ")\n",
    "from utils.metric_scripts import (\n",
    "    hello\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.*\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea891c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/final_project_data_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/_3/vjdmg_yj15j8km8wyn2prr8c0000gn/T/ipykernel_20967/2525481531.py:12: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load the latest version\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m  \u001b[49m\u001b[43mKaggleDatasetAdapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPANDAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdavidcariboo/player-scores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m  \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide any additional arguments like \u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# sql_query or pandas_kwargs. See the \u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# documenation for more information:\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst 5 records:\u001b[39m\u001b[33m\"\u001b[39m, df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/final_project_data_env/lib/python3.12/site-packages/kagglehub/datasets.py:178\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataset\u001b[39m(\n\u001b[32m    164\u001b[39m     adapter: KaggleDatasetAdapter,\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# In the form of {owner_slug}/{dataset_slug} or {owner_slug}/{dataset_slug}/versions/{version_number}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     hf_kwargs: Any = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    172\u001b[39m ) -> Any:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    173\u001b[39m     warnings.warn(\n\u001b[32m    174\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    175\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    176\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    177\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/final_project_data_env/lib/python3.12/site-packages/kagglehub/datasets.py:141\u001b[39m, in \u001b[36mdataset_load\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.PANDAS:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpandas_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_pandas_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.POLARS:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolars_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/final_project_data_env/lib/python3.12/site-packages/kagglehub/pandas_datasets.py:86\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     84\u001b[39m pandas_kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m pandas_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pandas_kwargs\n\u001b[32m     85\u001b[39m file_extension = os.path.splitext(path)[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m read_function = \u001b[43m_validate_read_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Now that everything has been validated, we can start downloading and processing\u001b[39;00m\n\u001b[32m     89\u001b[39m filepath = dataset_download(handle, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/final_project_data_env/lib/python3.12/site-packages/kagglehub/pandas_datasets.py:108\u001b[39m, in \u001b[36m_validate_read_function\u001b[39m\u001b[34m(file_extension, sql_query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_extension \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_READ_FUNCTIONS_BY_EXTENSION:\n\u001b[32m    104\u001b[39m     extension_error_message = (\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported file extension: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported file extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(extension_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    110\u001b[39m read_function = SUPPORTED_READ_FUNCTIONS_BY_EXTENSION[file_extension]\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read_function \u001b[38;5;129;01mis\u001b[39;00m wrapped_read_sql_query \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sql_query:\n",
      "\u001b[31mValueError\u001b[39m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
     ]
    }
   ],
   "source": [
    "# Importing dataset via Kaggle API \n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"davidcariboo/player-scores\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf8fc1",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization <a class=\"anchor\" id=\"data-vis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889b2d8",
   "metadata": {},
   "source": [
    "## Data Analytics <a class=\"anchor\" id=\"data-analytics\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
